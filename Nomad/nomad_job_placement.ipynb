{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8ee470ad-15ae-4290-9edf-e6bb50654532",
   "metadata": {},
   "source": [
    "# Nomad Advanced Job Placement"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a346b8e4-0656-4974-87a0-0b77cc4d1b52",
   "metadata": {},
   "source": [
    "teaser: |\n",
    "Explore advanced Nomad job placement strategies with Constraints, Affinities, and Spread.\n",
    "\n",
    "description: |-\n",
    "This track will show how you can control job placement in Nomad with:\n",
    "- [Constraints](https://www.nomadproject.io/docs/job-specification/constraint/)\n",
    "- [Affinities](https://www.nomadproject.io/docs/job-specification/affinity/)\n",
    "- and [Spread](https://www.nomadproject.io/docs/job-specification/spread/)\n",
    "- illustrating the flexibility of Nomad in this area.\n",
    "\n",
    "You will also learn about Nomad's [Variable Interpolation](https://www.nomadproject.io/docs/runtime/interpolation/) that allow applications deployed by Nomad to do things like use listen on ports dynamically selected by Nomad.\n",
    "\n",
    "You will deploy a Nomad cluster and run Nomad jobs that deploy a web application and [Traefik](https://containo.us/traefik/), which will provide load balancing across multiple instances of the application.\n",
    "\n",
    "Before running this track, we suggest you run the **Nomad Basics** and **Nomad Simple Cluster** tracks.\n",
    "\n",
    "<img src=https://storage.googleapis.com/instruqt-hashicorp-tracks/logo/nomad.png width=100>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90a76430-cab3-4568-8625-24d6db1e9eef",
   "metadata": {},
   "source": [
    "# Prep"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72343bd8-0355-4435-b61b-b16dcb757fdd",
   "metadata": {},
   "source": [
    "## AWS Credentials"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1535c140-da94-4462-b6d9-686865a60b67",
   "metadata": {},
   "source": [
    "Set your AWS Credentials. I got one from Instruqt terminal with this command.\n",
    "\n",
    "```bash\n",
    "env | grep -iE \"^aws.*access\" | xargs -I{} echo export {}\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d25f6063-6b61-44fe-be02-7d41432c7ec4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#==> Creds:\n",
      "us-west-2\n",
      "AKIAQZ6XIXZBVTQOWTUM\n",
      "1TuLFsGVoiBwD/p3JWjTG6c04Vyx76Af0R64k1+1\n"
     ]
    }
   ],
   "source": [
    "unset AWS_ACCESS_KEY_ID AWS_SECRET_ACCESS_KEY\n",
    "export AWS_DEFAULT_REGION=us-west-2\n",
    "export AWS_REGION=$AWS_DEFAULT_REGION\n",
    "export AWS_ACCESS_KEY_ID=\n",
    "export AWS_SECRET_ACCESS_KEY=\n",
    "export TF_VAR_aws_access_key_id=$AWS_ACCESS_KEY_ID\n",
    "export TF_VAR_aws_secret_access_key=$AWS_SECRET_ACCESS_KEY\n",
    "export TF_INPUT=false\n",
    "\n",
    "printf \"%s\\n\" \"#==> Creds:\" \"$AWS_REGION\" \"$AWS_ACCESS_KEY_ID\" \"$AWS_SECRET_ACCESS_KEY\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4e61f37-0867-4733-bef9-e23e9ac7f711",
   "metadata": {},
   "source": [
    "Create default VPC if needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d5425721-658a-4ea7-8ee1-719bd2795b7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "#==> Show VPC ids\n",
      "vpc-022ce4fd180ffc1af\n"
     ]
    }
   ],
   "source": [
    "aws configure set region us-west-2 --profile default\n",
    "aws ec2 create-default-vpc > /dev/null || true\n",
    "printf \"\\n#==> Show VPC ids\\n\"\n",
    "aws ec2 describe-vpcs | jq -r '.[] | .[] | .VpcId'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5528e4e-3ad2-4a50-9602-d36199ca0337",
   "metadata": {},
   "source": [
    "## Clone Repo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c1e9d43-3da6-477e-95a9-b86bce654705",
   "metadata": {},
   "outputs": [],
   "source": [
    "pushd /tmp >/dev/null\n",
    "git clone https://github.com/phanclan/nomad_terraform\n",
    "# cp -r /tmp/nomad/terraform /tmp/Nomad\n",
    "popd >/dev/null"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6c1113ac-79df-499d-9733-7ae01da5ea76",
   "metadata": {},
   "outputs": [],
   "source": [
    "# rm -rf /tmp/Nomad/{ssh_key,cluster} \n",
    "\n",
    "mkdir -p /tmp/nomad_terraform/ssh_key\n",
    "# mkdir -p /tmp/nomad_terraform/cluster"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e95caf06-df24-4e0e-a15c-b19fa26009ec",
   "metadata": {},
   "source": [
    "## create terragrunt.hcl - ssh_key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3f849c9b-0ab9-4510-a27a-4954bd510c8d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "cat > /tmp/nomad_terraform/ssh_key/terragrunt.hcl <<\"EOL\"\n",
    "# Indicate where to source the terraform module from.\n",
    "# The URL used here is a shorthand for\n",
    "# \"tfr://registry.terraform.io/terraform-aws-modules/vpc/aws?version=3.5.0\".\n",
    "# Note the extra `/` after the protocol is required for the shorthand\n",
    "# notation.\n",
    "terraform {\n",
    "  source = \"tfr:///cloudposse/key-pair/aws?version=0.18.3\"\n",
    "  extra_arguments \"plan\" {\n",
    "    commands = [\n",
    "      \"plan\",\n",
    "    ]\n",
    "    arguments = [\n",
    "      \"-input=false\",\n",
    "    ]\n",
    "  }\n",
    "  extra_arguments \"apply\" {\n",
    "    commands = [\n",
    "      \"apply\",\n",
    "      \"destroy\"\n",
    "    ]\n",
    "    arguments = [\n",
    "      \"-input=false\",\n",
    "      \"-auto-approve\"\n",
    "    ]\n",
    "  }\n",
    "}\n",
    "\n",
    "# generate \"versions\" {\n",
    "#   path = \"versions.tf\"\n",
    "#   if_exists = \"overwrite\"\n",
    "#   contents = <<EOF\n",
    "# terraform {\n",
    "#   # required_version = \"~1.1.0\"\n",
    "#   required_providers{\n",
    "#     aws = {\n",
    "#       source = \"hashicorp/aws\"\n",
    "#       version = \">= 2.70.0\"\n",
    "#     }\n",
    "#   }\n",
    "# }\n",
    "# EOF\n",
    "# }\n",
    "\n",
    "# generate \"provider\" {\n",
    "#   path = \"provider.tf\"\n",
    "#   if_exists = \"overwrite\"\n",
    "#   contents = <<EOF\n",
    "# provider \"aws\" {\n",
    "#   region = \"us-west-2\"   # region to deploy the resources into\n",
    "# }\n",
    "# EOF\n",
    "# }\n",
    "\n",
    "# Indicate the input values to use for the variables of the module.\n",
    "inputs = {\n",
    "  ssh_public_key_path       = \"/tmp/Nomad/ssh_key\"\n",
    "  generate_ssh_key     = true\n",
    "  name                 = \"aws-key-pair\"\n",
    "  # tags = {\n",
    "  #   Terraform   = \"true\"\n",
    "  #   Environment = \"root\"\n",
    "  #   Name        = \"Terragrunt-${path_relative_to_include()}\"\n",
    "  # }\n",
    "}\n",
    "\n",
    "EOL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "47da2dfe-c227-4888-92e5-a42ea69839cf",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "real\t0m12.590s\n",
      "user\t0m6.405s\n",
      "sys\t0m1.031s\n",
      "[1]+  Done                    time terragrunt apply > tf_apply_ssh_key_out.txt 2>&1\n"
     ]
    }
   ],
   "source": [
    "pushd /tmp/nomad_terraform/ssh_key >/dev/null\n",
    "time terragrunt apply > tf_apply_ssh_key_out.txt 2>&1 &\n",
    "popd >/dev/null"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7c332ddd-9518-4312-94f5-c58139661a8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARN\u001b[0m[0000] No double-slash (//) found in source URL /cloudposse/key-pair/aws. Relative paths in downloaded Terraform code may not work. \n",
      "/tmp/Nomad/ssh_key/aws-key-pair.pub"
     ]
    }
   ],
   "source": [
    "pushd /tmp/nomad_terraform/ssh_key >/dev/null\n",
    "terragrunt output -raw public_key_filename\n",
    "popd >/dev/null"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6df67e0-7c45-4a67-a741-f02c29d4207f",
   "metadata": {},
   "source": [
    "## create terragrunt.hcl - cluster"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8ee9fe6-adec-4483-a9f0-4e7585e86e82",
   "metadata": {},
   "source": [
    "### packer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f7713921-35e8-4fc2-a773-0d59f0e367e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0m\u001b[01;34menv\u001b[0m  \u001b[01;34mmodules\u001b[0m  packer.json  README.md\n"
     ]
    }
   ],
   "source": [
    "ls /tmp/nomad_terraform/aws"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "84d57aa2-c045-4262-95e3-e54c01c3980d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "real\t1m54.013s\n",
      "user\t0m0.476s\n",
      "sys\t0m0.316s\n",
      "[1]+  Exit 1                  time packer build packer.json > /tmp/packer_nomad_out.txt 2>&1\n",
      "{\n",
      "  \"builders\": [{\n",
      "    \"type\": \"amazon-ebs\",\n",
      "    \"region\": \"us-west-2\",\n",
      "    \"source_ami_filter\": {\n",
      "      \"filters\": {\n",
      "        \"virtualization-type\": \"hvm\",\n",
      "        \"architecture\": \"x86_64\",\n",
      "        \"name\": \"ubuntu/images/hvm-ssd/ubuntu-xenial-16.04-amd64-server-*\",\n",
      "        \"block-device-mapping.volume-type\": \"gp2\",\n",
      "        \"root-device-type\": \"ebs\"\n",
      "      },\n",
      "      \"owners\": [\"099720109477\"],\n",
      "      \"most_recent\": true\n",
      "    },\n",
      "    \"instance_type\": \"t3.large\",\n",
      "    \"ssh_username\": \"ubuntu\",\n",
      "    \"ami_name\": \"hashistack {{timestamp}}\"\n",
      "  }],\n",
      "  \"provisioners\":  [\n",
      "  {\n",
      "    \"type\": \"shell\",\n",
      "    \"inline\": [\n",
      "      \"sudo mkdir /ops\",\n",
      "      \"sudo chmod 777 /ops\"\n",
      "    ]\n",
      "  },\n",
      "  {\n",
      "    \"type\": \"file\",\n",
      "    \"source\": \"../shared\",\n",
      "    \"destination\": \"/ops\"\n",
      "  },\n",
      "  {\n",
      "    \"type\": \"file\",\n",
      "    \"source\": \"../examples\",\n",
      "    \"destination\": \"/ops\"\n",
      "  },\n",
      "  {\n",
      "    \"type\": \"shell\",\n",
      "    \"script\": \"../shared/scripts/setup.sh\",\n",
      "    \"environment_vars\": [\n",
      "      \"INSTALL_NVIDIA_DOCKER=true\"\n",
      "    ]\n",
      "  }]\n",
      "}\n",
      "[1] 9554\n"
     ]
    }
   ],
   "source": [
    "pushd /tmp/nomad_terraform/aws >/dev/null\n",
    "tee packer.json <<EOL\n",
    "{\n",
    "  \"builders\": [{\n",
    "    \"type\": \"amazon-ebs\",\n",
    "    \"region\": \"us-west-2\",\n",
    "    \"source_ami_filter\": {\n",
    "      \"filters\": {\n",
    "        \"virtualization-type\": \"hvm\",\n",
    "        \"architecture\": \"x86_64\",\n",
    "        \"name\": \"ubuntu/images/hvm-ssd/ubuntu-xenial-16.04-amd64-server-*\",\n",
    "        \"block-device-mapping.volume-type\": \"gp2\",\n",
    "        \"root-device-type\": \"ebs\"\n",
    "      },\n",
    "      \"owners\": [\"099720109477\"],\n",
    "      \"most_recent\": true\n",
    "    },\n",
    "    \"instance_type\": \"t3.large\",\n",
    "    \"ssh_username\": \"ubuntu\",\n",
    "    \"ami_name\": \"hashistack {{timestamp}}\"\n",
    "  }],\n",
    "  \"provisioners\":  [\n",
    "  {\n",
    "    \"type\": \"shell\",\n",
    "    \"inline\": [\n",
    "      \"sudo mkdir /ops\",\n",
    "      \"sudo chmod 777 /ops\"\n",
    "    ]\n",
    "  },\n",
    "  {\n",
    "    \"type\": \"file\",\n",
    "    \"source\": \"../shared\",\n",
    "    \"destination\": \"/ops\"\n",
    "  },\n",
    "  {\n",
    "    \"type\": \"file\",\n",
    "    \"source\": \"../examples\",\n",
    "    \"destination\": \"/ops\"\n",
    "  },\n",
    "  {\n",
    "    \"type\": \"shell\",\n",
    "    \"script\": \"../shared/scripts/setup.sh\",\n",
    "    \"environment_vars\": [\n",
    "      \"INSTALL_NVIDIA_DOCKER=true\"\n",
    "    ]\n",
    "  }]\n",
    "}\n",
    "EOL\n",
    "\n",
    "time packer build packer.json > /tmp/packer_nomad_out.txt 2>&1 &\n",
    "popd >/dev/null"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "3baa161b-a8c8-4b4d-933b-6d2c6eafd053",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ami-0753fcee97d3a73ad\n"
     ]
    }
   ],
   "source": [
    "tail -n 30 /tmp/packer_nomad_out.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e22a3f2d-a9b0-4fa8-b28e-111a637eaf32",
   "metadata": {},
   "source": [
    "### terraform configs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4c4ec2b-a717-4da8-b20c-8cfabdd19d2d",
   "metadata": {},
   "source": [
    "Get the ami"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "d69007f2-0329-4531-ae6a-94fdf86a6d8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ami-0753fcee97d3a73ad\n"
     ]
    }
   ],
   "source": [
    "grep ami /tmp/packer_nomad_out.txt | tail -n 1 | awk '{print $NF}'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbf8107a-d829-4c9e-893f-1efd38ef9e6d",
   "metadata": {},
   "source": [
    "Customize the inputs below. Use the `ami` value above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "a91750b9-cde2-4699-8d74-38ab1c23c4ee",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    }
   ],
   "source": [
    "cat > /tmp/nomad_terraform/aws/env/terragrunt.hcl <<\"EOL\"\n",
    "# Indicate where to source the terraform module from.\n",
    "# The URL used here is a shorthand for\n",
    "# \"tfr://registry.terraform.io/terraform-aws-modules/vpc/aws?version=3.5.0\".\n",
    "# Note the extra `/` after the protocol is required for the shorthand\n",
    "# notation.\n",
    "terraform {\n",
    "  #source = \"git::https://github.com/hashicorp/nomad.git//terraform/aws/modules/hashistack\"\n",
    "  extra_arguments \"plan\" {\n",
    "    commands = [\n",
    "      \"plan\",\n",
    "    ]\n",
    "    arguments = [\n",
    "      \"-input=false\",\n",
    "    ]\n",
    "  }\n",
    "  extra_arguments \"apply\" {\n",
    "    commands = [\n",
    "      \"apply\",\n",
    "      \"destroy\",\n",
    "    ]\n",
    "    arguments = [\n",
    "      \"-input=false\",\n",
    "      \"-auto-approve\"\n",
    "    ]\n",
    "  }\n",
    "}\n",
    "\n",
    "generate \"versions\" {\n",
    "  path = \"versions.tf\"\n",
    "  if_exists = \"overwrite\"\n",
    "  contents = <<EOF\n",
    "terraform {\n",
    "  # required_version = \"~1.1.0\"\n",
    "  required_providers{\n",
    "    aws = {\n",
    "      source = \"hashicorp/aws\"\n",
    "      version = \"~> 3.75.2\"\n",
    "    }\n",
    "  }\n",
    "}\n",
    "EOF\n",
    "}\n",
    "\n",
    "generate \"provider\" {\n",
    "  path = \"provider.tf\"\n",
    "  if_exists = \"overwrite\"\n",
    "  contents = <<EOF\n",
    "provider \"aws\" {\n",
    "  region = \"us-west-2\"   # region to deploy the resources into\n",
    "}\n",
    "EOF\n",
    "}\n",
    "\n",
    "#// Indicate the input values to use for the variables of the module.\n",
    "inputs = {\n",
    "  name         = \"pphan\"\n",
    "  region        = \"us-west-2\"\n",
    "  ami           = \"ami-0753fcee97d3a73ad\"\n",
    "  server_instance_type  = \"m5.large\"\n",
    "  client_instance_type  = \"m5.large\"\n",
    "  client_count  = 3\n",
    "  server_count  = 3\n",
    "  key_name      = \"aws-key-pair\"\n",
    "  whitelist_ip = \"98.234.158.216/32\"\n",
    "  root_block_device_size = 16\n",
    "  nomad_binary           = \"none\"\n",
    "#   consul_version = \"1.13.1\"\n",
    "#   nomad_version  = \"1.3.5\"\n",
    "#   owner          = \"pphan\"\n",
    "#   vpc_id         = \"vpc-0a3da3e09494785db\"\n",
    "#   #//optional\n",
    "#   public_ip      = true\n",
    "#   #consul_config = {}\n",
    "#   tags = {\n",
    "#     Terraform   = \"true\"\n",
    "#     Environment = \"root\"\n",
    "#     Name        = \"Terragrunt-${path_relative_to_include()}\"\n",
    "#   }\n",
    "}\n",
    "\n",
    "EOL\n",
    "echo done"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "caca99ef-3a38-476e-8be6-e34ea2793edb",
   "metadata": {},
   "outputs": [],
   "source": [
    "cat > /tmp/nomad_terraform/aws/env/main.tf <<\"EOL\"\n",
    "module \"hashistack\" {\n",
    "  source = \"../modules/hashistack\"\n",
    "  name                   = var.name\n",
    "  region                 = var.region\n",
    "  ami                    = var.ami\n",
    "  server_instance_type   = var.server_instance_type\n",
    "  client_instance_type   = var.client_instance_type\n",
    "  key_name               = var.key_name\n",
    "  server_count           = var.server_count\n",
    "  client_count           = var.client_count\n",
    "  retry_join             = var.retry_join\n",
    "  nomad_binary           = var.nomad_binary\n",
    "  root_block_device_size = var.root_block_device_size\n",
    "  whitelist_ip           = var.whitelist_ip\n",
    "}\n",
    "\n",
    "variable \"name\" {\n",
    "  description = \"Used to name various infrastructure components\"\n",
    "}\n",
    "\n",
    "variable \"whitelist_ip\" {\n",
    "  description = \"IP to whitelist for the security groups (set 0.0.0.0/0 for world)\"\n",
    "}\n",
    "\n",
    "variable \"region\" {}\n",
    "\n",
    "variable \"ami\" {}\n",
    "\n",
    "variable \"server_instance_type\" {}\n",
    "\n",
    "variable \"client_instance_type\" {}\n",
    "\n",
    "variable \"root_block_device_size\" {}\n",
    "\n",
    "variable \"key_name\" {}\n",
    "\n",
    "variable \"server_count\" {}\n",
    "\n",
    "variable \"client_count\" {}\n",
    "\n",
    "variable \"retry_join\" {\n",
    "  type = map(string)\n",
    "  default = {\n",
    "    provider  = \"aws\"\n",
    "    tag_key   = \"ConsulAutoJoin\"\n",
    "    tag_value = \"auto-join\"\n",
    "  }\n",
    "}\n",
    "\n",
    "variable \"nomad_binary\" {}\n",
    "EOL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "374909fa-4eda-4bf7-bc5a-51b38f26f214",
   "metadata": {},
   "outputs": [],
   "source": [
    "cat > /tmp/nomad_terraform/aws/env/outputs.tf <<\"EOL\"\n",
    "output \"IP_Addresses\" {\n",
    "  value = <<CONFIGURATION\n",
    "Client public IPs: ${join(\", \", module.hashistack.client_public_ips)}\n",
    "Server public IPs: ${join(\", \", module.hashistack.server_public_ips)}\n",
    "To connect, add your private key and SSH into any client or server with\n",
    "`ssh ubuntu@PUBLIC_IP`. You can test the integrity of the cluster by running:\n",
    "  $ consul members\n",
    "  $ nomad server members\n",
    "  $ nomad node status\n",
    "If you see an error message like the following when running any of the above\n",
    "commands, it usually indicates that the configuration script has not finished\n",
    "executing:\n",
    "\"Error querying servers: Get http://127.0.0.1:4646/v1/agent/members: dial tcp\n",
    "127.0.0.1:4646: getsockopt: connection refused\"\n",
    "Simply wait a few seconds and rerun the command if this occurs.\n",
    "The Nomad UI can be accessed at http://${module.hashistack.server_lb_ip}:4646/ui.\n",
    "The Consul UI can be accessed at http://${module.hashistack.server_lb_ip}:8500/ui.\n",
    "Set the following for access from the Nomad CLI:\n",
    "  export NOMAD_ADDR=http://${module.hashistack.server_lb_ip}:4646\n",
    "CONFIGURATION\n",
    "}\n",
    "output \"consul_http_addr\" {\n",
    "  value = \"http://${module.hashistack.server_lb_ip}:8500\"\n",
    "}\n",
    "output \"nomad_addr\" {\n",
    "  value = \"http://${module.hashistack.server_lb_ip}:4646\"\n",
    "}\n",
    "output \"client_public_ips_2\" {\n",
    "  value = module.hashistack.client_public_ips_2\n",
    "}\n",
    "EOL"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59a8c574-abcc-46a8-9c6a-3417bffb8769",
   "metadata": {},
   "source": [
    "#### extra client"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0680cc46-3c19-463a-80a2-f9541ddd46d6",
   "metadata": {},
   "source": [
    "- https://aws.amazon.com/ec2/spot/pricing/\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "0a1691c5-68fa-44c4-ba1c-bee0c89549aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "cat > /tmp/nomad_terraform/aws/modules/hashistack/client2.tf <<\"EOL\"\n",
    "resource \"aws_spot_instance_request\" \"client2\" {\n",
    "  ami                    = var.ami\n",
    "  spot_price             = \"0.14\"\n",
    "  instance_type          = \"m5.2xlarge\"\n",
    "  key_name               = var.key_name\n",
    "  vpc_security_group_ids = [aws_security_group.primary.id]\n",
    "  count                  = 1\n",
    "  depends_on             = [aws_instance.server]\n",
    "  ipv6_address_count     = 0\n",
    "  ipv6_addresses         = []\n",
    "\n",
    "  # instance tags\n",
    "  tags = merge(\n",
    "    {\n",
    "      \"Name\" = \"${var.name}-client-${count.index}\"\n",
    "    },\n",
    "    {\n",
    "      \"${var.retry_join.tag_key}\" = \"${var.retry_join.tag_value}\"\n",
    "    },\n",
    "  )\n",
    "\n",
    "  root_block_device {\n",
    "    volume_type           = \"gp3\"\n",
    "    volume_size           = var.root_block_device_size\n",
    "    delete_on_termination = \"true\"\n",
    "  }\n",
    "\n",
    "  ebs_block_device {\n",
    "    device_name           = \"/dev/xvdd\"\n",
    "    volume_type           = \"gp3\"\n",
    "    volume_size           = \"50\"\n",
    "    delete_on_termination = \"true\"\n",
    "  }\n",
    "\n",
    "  user_data = templatefile(\"${path.root}/user-data-client.sh\",\n",
    "    {\n",
    "      region = var.region\n",
    "      retry_join = chomp(\n",
    "        join(\n",
    "          \" \",\n",
    "          formatlist(\"%s=%s \", keys(var.retry_join), values(var.retry_join)),\n",
    "        ),\n",
    "      )\n",
    "      nomad_binary = var.nomad_binary\n",
    "    }\n",
    "  )\n",
    "  iam_instance_profile = aws_iam_instance_profile.instance_profile.name\n",
    "}\n",
    "output \"client_public_ips_2\" {\n",
    "  value = aws_spot_instance_request.client2[*].public_ip\n",
    "}\n",
    "EOL"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "528aa30b-78fe-4c7d-a22b-53df67cbe8cb",
   "metadata": {},
   "source": [
    "### copy user-data scripts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "dd315ede-cd36-4ecd-b6df-718ef8a4c4c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "pushd /tmp/nomad_terraform/aws/env >/dev/null\n",
    "cp ./us-east/user-data-*.sh .\n",
    "popd >/dev/null"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84b35a25-4573-476c-ba96-9ff82c13db6c",
   "metadata": {},
   "source": [
    "### terraform init"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "a475c96c-b37b-4712-b9c7-b2fc95449563",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0m\u001b[1mUpgrading modules...\u001b[0m\n",
      "- hashistack in ../modules/hashistack\n",
      "\n",
      "\u001b[0m\u001b[1mInitializing the backend...\u001b[0m\n",
      "\n",
      "\u001b[0m\u001b[1mInitializing provider plugins...\u001b[0m\n",
      "- Finding hashicorp/aws versions matching \"~> 3.75.2\"...\n",
      "- Using previously-installed hashicorp/aws v3.75.2\n",
      "\n",
      "\u001b[0m\u001b[1m\u001b[32mTerraform has been successfully initialized!\u001b[0m\u001b[32m\u001b[0m\n",
      "\u001b[0m\u001b[32m\n",
      "You may now begin working with Terraform. Try running \"terraform plan\" to see\n",
      "any changes that are required for your infrastructure. All Terraform commands\n",
      "should now work.\n",
      "\n",
      "If you ever set or change modules or backend configuration for Terraform,\n",
      "rerun this command to reinitialize your working directory. If you forget, other\n",
      "commands will detect it and remind you to do so if necessary.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "pushd /tmp/nomad_terraform/aws/env >/dev/null\n",
    "terragrunt init -upgrade -force-copy\n",
    "popd >/dev/null"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a471312-f45f-4868-afdd-664ea4283fa5",
   "metadata": {},
   "source": [
    "### terraform apply"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "84b015a1-a1cb-419b-8a06-3b9ecc352b17",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/tmp/nomad_terraform/aws/env /media/code/hc_demos-jupyter/Nomad\n",
      "[1] 11798\n",
      "/media/code/hc_demos-jupyter/Nomad\n"
     ]
    }
   ],
   "source": [
    "pushd /tmp/nomad_terraform/aws/env\n",
    "time terragrunt apply > /tmp/tf_apply_nomad_out.txt 2>&1 &\n",
    "# time terragrunt refresh\n",
    "popd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "6803e332-2e13-4367-9d4a-c2e1ea79d1f3",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          \u001b[32m+\u001b[0m \u001b[0m\u001b[1m\u001b[0mvolume_id\u001b[0m\u001b[0m             = \"vol-07ca49affc3ccdb56\"\n",
      "            \u001b[90m# (4 unchanged attributes hidden)\u001b[0m\u001b[0m\n",
      "        }\n",
      "    }\n",
      "\n",
      "\n",
      "Unless you have made equivalent changes to your configuration, or ignored the\n",
      "relevant attributes using ignore_changes, the following plan may include\n",
      "actions to undo or respond to these changes.\n",
      "\u001b[90m\n",
      "─────────────────────────────────────────────────────────────────────────────\u001b[0m\n",
      "\u001b[0m\n",
      "\u001b[1mChanges to Outputs:\u001b[0m\u001b[0m\n",
      "  \u001b[33m~\u001b[0m \u001b[0m\u001b[1m\u001b[0mclient_public_ips_2\u001b[0m\u001b[0m = [\n",
      "      \u001b[31m-\u001b[0m \u001b[0m\u001b[90mnull\u001b[0m\u001b[0m,\n",
      "      \u001b[32m+\u001b[0m \u001b[0m\"35.90.197.196\",\n",
      "    ]\n",
      "\n",
      "You can apply this plan to save these new output values to the Terraform\n",
      "state, without changing any real infrastructure.\n",
      "\u001b[0m\u001b[1m\u001b[32m\n",
      "Apply complete! Resources: 0 added, 0 changed, 0 destroyed.\n",
      "\u001b[0m\u001b[0m\u001b[1m\u001b[32m\n",
      "Outputs:\n",
      "\n",
      "\u001b[0mIP_Addresses = <<EOT\n",
      "Client public IPs: 35.90.27.18, 34.216.150.37, 34.217.94.196\n",
      "Server public IPs: 35.89.162.19, 54.212.87.37, 54.244.58.182\n",
      "To connect, add your private key and SSH into any client or server with\n",
      "`ssh ubuntu@PUBLIC_IP`. You can test the integrity of the cluster by running:\n",
      "  $ consul members\n",
      "  $ nomad server members\n",
      "  $ nomad node status\n",
      "If you see an error message like the following when running any of the above\n",
      "commands, it usually indicates that the configuration script has not finished\n",
      "executing:\n",
      "\"Error querying servers: Get http://127.0.0.1:4646/v1/agent/members: dial tcp\n",
      "127.0.0.1:4646: getsockopt: connection refused\"\n",
      "Simply wait a few seconds and rerun the command if this occurs.\n",
      "The Nomad UI can be accessed at http://pphan-server-lb-992086560.us-west-2.elb.amazonaws.com:4646/ui.\n",
      "The Consul UI can be accessed at http://pphan-server-lb-992086560.us-west-2.elb.amazonaws.com:8500/ui.\n",
      "Set the following for access from the Nomad CLI:\n",
      "  export NOMAD_ADDR=http://pphan-server-lb-992086560.us-west-2.elb.amazonaws.com:4646\n",
      "\n",
      "EOT\n",
      "client_public_ips_2 = [\n",
      "  \"35.90.197.196\",\n",
      "]\n",
      "consul_http_addr = \"http://pphan-server-lb-992086560.us-west-2.elb.amazonaws.com:8500\"\n",
      "nomad_addr = \"http://pphan-server-lb-992086560.us-west-2.elb.amazonaws.com:4646\"\n"
     ]
    }
   ],
   "source": [
    "tail -n 50 /tmp/tf_apply_nomad_out.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74afaf3d-6705-4dba-8c07-becf3a760fd5",
   "metadata": {},
   "source": [
    "## set nomad and consul variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "0b302033-f542-426d-ab8e-d6007ee8b298",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nomad UI: http://pphan-server-lb-992086560.us-west-2.elb.amazonaws.com:4646\n",
      "Consul UI:http://pphan-server-lb-992086560.us-west-2.elb.amazonaws.com:8500\n"
     ]
    }
   ],
   "source": [
    "pushd /tmp/nomad_terraform/aws/env >/dev/null\n",
    "export NOMAD_ADDR=$(terragrunt output -raw nomad_addr)\n",
    "export CONSUL_HTTP_ADDR=$(terragrunt output -raw consul_http_addr)\n",
    "# export NOMAD_ADDR=http://pphan-server-lb-2092905469.us-west-2.elb.amazonaws.com:4646\n",
    "# export CONSUL_HTTP_ADDR=http://pphan-server-lb-2092905469.us-west-2.elb.amazonaws.com:8500\n",
    "printf \"%s\\n\" \"Nomad UI: $NOMAD_ADDR\" \"Consul UI:$CONSUL_HTTP_ADDR\"\n",
    "popd >/dev/null"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "5243f268-fd30-4d5c-b01e-2f751e700047",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "35.90.197.196\n"
     ]
    }
   ],
   "source": [
    "pushd /tmp/nomad_terraform/aws/env >/dev/null\n",
    "terragrunt output -json client_public_ips_2 | jq -r .[0]\n",
    "# export NOMAD_ADDR=$(terragrunt output -raw nomad_addr)\n",
    "# export NOMAD_ADDR=http://pphan-server-lb-2092905469.us-west-2.elb.amazonaws.com:4646\n",
    "# printf \"%s\\n\" \"Nomad UI: $NOMAD_ADDR\" \"Consul UI:$CONSUL_HTTP_ADDR\"\n",
    "popd >/dev/null"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ab2cb7e-f5f9-4c93-b4fe-e1767a505543",
   "metadata": {},
   "source": [
    "- slug: verify-nomad-cluster-health"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92c4f147-76a6-4d3c-a38e-17d3401b2388",
   "metadata": {},
   "source": [
    "# Verify the Health of Your Nomad Cluster"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00f80b02-7300-4f76-910c-11186fadb1c0",
   "metadata": {
    "tags": []
   },
   "source": [
    "teaser: |\n",
    "    Verify the health of the Nomad cluster that has been deployed for you.\n",
    "\n",
    "## notes:\n",
    "\n",
    "In this challenge, you will verify the health of the Nomad cluster that has been deployed for you by the track's setup scripts. This will include checking the health of a Consul cluster that has been set up on the same VMs.\n",
    "\n",
    "In later challenges, you will run Nomad jobs that deploy a web application and the Traefik load balancer. You will then update them using Nomad's various options for controlling job placement.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23f293b5-e809-4fb9-8d01-c1121b59fc2c",
   "metadata": {},
   "source": [
    "In this challenge, you will verify the health of the Nomad cluster that has been deployed for you by the track's setup scripts. This will include checking the health of a Consul cluster that has been set up on the same VMs.\n",
    "\n",
    "The cluster is running:\n",
    "- 3 Nomad/Consul server\n",
    "- 3 Nomad/Consul clients.\n",
    "\n",
    "They are using software versions:\n",
    "- Nomad 1.3.1\n",
    "- Consul 1.12.2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cb97be6-8a41-4c76-8cc5-fd4bfa4c5562",
   "metadata": {},
   "source": [
    "First, verify that all 6 Consul agents are running and connected to the cluster:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "afc1a5d3-c4a0-4327-8d31-2792820c6d42",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Node              Address             Status  Type    Build   Protocol  DC   Partition  Segment\n",
      "ip-172-31-35-254  172.31.35.254:8301  alive   server  1.12.2  2         dc1  default    <all>\n",
      "ip-172-31-38-221  172.31.38.221:8301  alive   server  1.12.2  2         dc1  default    <all>\n",
      "ip-172-31-38-99   172.31.38.99:8301   alive   server  1.12.2  2         dc1  default    <all>\n",
      "ip-172-31-2-150   172.31.2.150:8301   alive   client  1.12.2  2         dc1  default    <default>\n",
      "ip-172-31-33-53   172.31.33.53:8301   alive   client  1.12.2  2         dc1  default    <default>\n",
      "ip-172-31-36-101  172.31.36.101:8301  alive   client  1.12.2  2         dc1  default    <default>\n",
      "ip-172-31-36-154  172.31.36.154:8301  alive   client  1.12.2  2         dc1  default    <default>\n"
     ]
    }
   ],
   "source": [
    "consul members"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe744766-350d-4a97-bceb-f55ba28ee7a8",
   "metadata": {},
   "source": [
    "You should see 6 Consul agents with the \"`alive`\" status.\n",
    "\n",
    "```\n",
    "Node              Address             Status  Type    Build   Protocol  DC   Partition  Segment\n",
    "ip-172-31-32-161  172.31.32.161:8301  alive   server  1.12.2  2         dc1  default    <all>\n",
    "ip-172-31-35-92   172.31.35.92:8301   alive   server  1.12.2  2         dc1  default    <all>\n",
    "ip-172-31-46-21   172.31.46.21:8301   alive   server  1.12.2  2         dc1  default    <all>\n",
    "ip-172-31-36-155  172.31.36.155:8301  alive   client  1.12.2  2         dc1  default    <default>\n",
    "ip-172-31-38-164  172.31.38.164:8301  alive   client  1.12.2  2         dc1  default    <default>\n",
    "ip-172-31-46-251  172.31.46.251:8301  alive   client  1.12.2  2         dc1  default    <default>\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05b4758f-20f7-4185-adf0-8467f2f7b75b",
   "metadata": {},
   "source": [
    "Check that the Nomad server is running:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "3cff1cfa-adf7-4e0a-9216-9bf9a1c1ffc3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0mName                     Address        Port  Status  Leader  Raft Version  Build  Datacenter  Region\n",
      "ip-172-31-35-254.global  172.31.35.254  4648  alive   false   3             1.3.1  dc1         global\n",
      "ip-172-31-38-221.global  172.31.38.221  4648  alive   true    3             1.3.1  dc1         global\n",
      "ip-172-31-38-99.global   172.31.38.99   4648  alive   false   3             1.3.1  dc1         global\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "nomad server members"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4131db0c-1f04-4be3-9f3b-d708872203b1",
   "metadata": {},
   "source": [
    "You should see 3 Nomad servers with the \"`alive`\" status.\n",
    "```\n",
    "Name                     Address        Port  Status  Leader  Raft Version  Build  Datacenter  Region\n",
    "ip-172-31-32-161.global  172.31.32.161  4648  alive   false   3             1.3.1  dc1         global\n",
    "ip-172-31-35-92.global   172.31.35.92   4648  alive   false   3             1.3.1  dc1         global\n",
    "ip-172-31-46-21.global   172.31.46.21   4648  alive   true    3             1.3.1  dc1         global\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27ae8020-4a13-4072-81aa-88d5114b1c2b",
   "metadata": {},
   "source": [
    "Check the status of the Nomad client nodes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "a29d7a1d-0762-406f-a67c-022ba091a1d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0mID        DC   Name              Class   Drain  Eligibility  Status\n",
      "086e619b  dc1  ip-172-31-2-150   <none>  false  eligible     ready\n",
      "22f005c9  dc1  ip-172-31-36-154  <none>  false  eligible     ready\n",
      "84c13196  dc1  ip-172-31-36-101  <none>  false  eligible     ready\n",
      "160234a4  dc1  ip-172-31-33-53   <none>  false  eligible     ready\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "nomad node status"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63b77ed5-b136-436d-9776-3e3d35fa33bb",
   "metadata": {},
   "source": [
    "You should see 3 Nomad clients with the \"`ready`\" status.\n",
    "\n",
    "```\n",
    "ID        DC   Name              Class   Drain  Eligibility  Status\n",
    "ba90fa7e  dc1  ip-172-31-36-155  <none>  false  eligible     ready\n",
    "48d0b218  dc1  ip-172-31-38-164  <none>  false  eligible     ready\n",
    "6f750cb2  dc1  ip-172-31-46-251  <none>  false  eligible     ready\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f4caa52-4460-4aa1-9d81-79b8f11dfcf6",
   "metadata": {},
   "source": [
    "You can also check the status of the Nomad server and clients in the Nomad and Consul UIs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "bd8cafdd-29c8-49bd-bf3b-a9d8137bd8fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Consul UI: http://pphan-server-lb-992086560.us-west-2.elb.amazonaws.com:8500\n",
      "Nomad UI: http://pphan-server-lb-992086560.us-west-2.elb.amazonaws.com:4646\n"
     ]
    }
   ],
   "source": [
    "printf \"%s\\n\" \"Consul UI: $CONSUL_HTTP_ADDR\" \"Nomad UI: $NOMAD_ADDR\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "245b7691-f673-4647-8eb2-0ea5301979c6",
   "metadata": {},
   "source": [
    "In the next challenge, you will run jobs that deploy a web application and the Traefik load balancer.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b7b1153-6993-4936-a4f1-b8e295d935a5",
   "metadata": {},
   "source": [
    "- slug: deploy-the-jobs\n",
    "\n",
    "# Deploy a Web Application and Traefik with Nomad"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41ac6e3b-ca75-44c3-ae15-79eef9435009",
   "metadata": {},
   "source": [
    "teaser: |\n",
    "Deploy a web application and Traefik with Nomad jobs.\n",
    "\n",
    "## notes:\n",
    "\n",
    "In this challenge, you will run Nomad jobs that deploy a web application and [Traefik](https://containo.us/traefik/), which will serve as a load balancer in front of multiple instances of the web app.\n",
    "\n",
    "In later challenges, you will learn about Nomad Spread, Constraints, and Affinities.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f678f3b-03a8-420a-a490-9098dc02e707",
   "metadata": {},
   "source": [
    "In this challenge, you will run two Nomad jobs:\n",
    "* The first will deploy 6 instances of a web app.\n",
    "* The second will run Traefik as a load balancer for the web app."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a49b531b-13c6-4a6c-bffe-e951b622465a",
   "metadata": {},
   "source": [
    "## Inspect the webapp.nomad Job.\n",
    "\n",
    "Let's begin by inspecting the Nomad jobs and getting familiar with what you're going to deploy."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d72465ac-f05c-4171-8948-374bcba2072b",
   "metadata": {},
   "source": [
    "Inspect the \"`webapp.nomad`\" job specification file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "e71c45d0-ebca-4c0e-a0dd-4ccd31c9d904",
   "metadata": {},
   "outputs": [],
   "source": [
    "mkdir -p /tmp/nomad_terraform/jobs\n",
    "cat > /tmp/nomad_terraform/jobs/webapp.nomad <<-EOF\n",
    "job \"webapp\" {\n",
    "  datacenters = [\"dc1\"]\n",
    "  group \"webapp\" {\n",
    "    count = 6\n",
    "    network {\n",
    "      port  \"http\" {}\n",
    "    }\n",
    "    task \"server\" {\n",
    "      env {\n",
    "        PORT    = \"\\${NOMAD_PORT_http}\"\n",
    "        NODE_IP = \"\\${NOMAD_IP_http}\"\n",
    "      }\n",
    "      driver = \"docker\"\n",
    "      config {\n",
    "        image = \"hashicorp/demo-webapp-lb-guide\"\n",
    "        ports = [\"http\"]\n",
    "      }\n",
    "      resources {\n",
    "        cpu    = 20\n",
    "        memory = 678\n",
    "      }\n",
    "      service {\n",
    "        name = \"webapp\"\n",
    "        port = \"http\"\n",
    "        tags = [\n",
    "          \"traefik.tags=service\",\n",
    "          \"traefik.frontend.rule=PathPrefixStrip:/myapp\",\n",
    "        ]\n",
    "        check {\n",
    "          type     = \"http\"\n",
    "          path     = \"/\"\n",
    "          interval = \"2s\"\n",
    "          timeout  = \"2s\"\n",
    "        }\n",
    "      }\n",
    "    }\n",
    "  }\n",
    "}\n",
    "EOF"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa55a13a-6a1a-40f5-b70a-c8bdc4fa7f14",
   "metadata": {},
   "source": [
    "- This will deploy 6 instances of our web app to your Nomad cluster since the `count` of the \"`webapp`\" task group is set to 6.\n",
    "- Note, however, that we have not yet used any of the job placement stanzas mentioned in this track's description.\n",
    "    - So, Nomad is free to place the 6 instances wherever it wants.\n",
    "\n",
    "Since the job specification does not specify a static port to use, Nomad will select a dynamic port for each web app instance.\n",
    "- This allows us to run more than one instance of the web app on each Nomad client.\n",
    "- In contrast, if we had specified a static port, we could only have run one instance per Nomad client.\n",
    "\n",
    "Since we are using dynamic ports, each instance of the web app has to listen on the right port.\n",
    "- The job enables them to do that with [variable interpolation](https://nomadproject.io/docs/runtime/interpolation/)\n",
    "- the job sets the `PORT` and `NODE_IP` environment variables to `${NOMAD_PORT_http}` and `${NOMAD_IP_http}` respectively.\n",
    "- When each instance of the web app starts, it can read those environment variables and bind to the correct IP and port.\n",
    "    - This is achieved in combination with the [port parameters](https://nomadproject.io/docs/job-specification/network/#port-parameters) in the network stanza of the job specification."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98495c08-9d4d-4ce6-86f2-b19f9bccd990",
   "metadata": {},
   "source": [
    "## Run the webapp.nomad Job"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10afde63-ab17-405a-9041-9010c9bcf8f2",
   "metadata": {},
   "source": [
    "Navigate to the `/tmp/nomad_terraform/jobs` directory:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "f61394a9-65c2-41c2-b8c6-28316da0fbe5",
   "metadata": {},
   "outputs": [],
   "source": [
    "cd /tmp/nomad_terraform/jobs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89a2af81-a4f1-40af-9a4d-d5c9457ff248",
   "metadata": {},
   "source": [
    "Run the \"`webapp.nomad`\" job with this command on the \"`Server`\" tab:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "7e531d2d-68e9-4809-9a49-ff5f47fcf922",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1] 12362\n"
     ]
    }
   ],
   "source": [
    "nomad job run webapp.nomad > /tmp/nomad_job_run.txt 2>&1 &"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "5e759222-6892-4d02-a9eb-c0d0edb2a7cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==> 2022-09-07T21:50:16-07:00: Monitoring evaluation \"e3b43658\"\n",
      "    2022-09-07T21:50:16-07:00: Evaluation triggered by job \"webapp\"\n",
      "    2022-09-07T21:50:16-07:00: Evaluation within deployment: \"d08ba604\"\n",
      "    2022-09-07T21:50:16-07:00: Allocation \"81407a40\" created: node \"160234a4\", group \"webapp\"\n",
      "    2022-09-07T21:50:16-07:00: Allocation \"a821c6ba\" created: node \"22f005c9\", group \"webapp\"\n",
      "    2022-09-07T21:50:16-07:00: Allocation \"f2906fdc\" created: node \"84c13196\", group \"webapp\"\n",
      "    2022-09-07T21:50:16-07:00: Allocation \"33d4b95f\" created: node \"086e619b\", group \"webapp\"\n",
      "    2022-09-07T21:50:16-07:00: Allocation \"464cd457\" created: node \"160234a4\", group \"webapp\"\n",
      "    2022-09-07T21:50:16-07:00: Allocation \"577157b5\" created: node \"160234a4\", group \"webapp\"\n",
      "    2022-09-07T21:50:16-07:00: Evaluation status changed: \"pending\" -> \"complete\"\n",
      "==> 2022-09-07T21:50:16-07:00: Evaluation \"e3b43658\" finished with status \"complete\"\n",
      "==> 2022-09-07T21:50:16-07:00: Monitoring deployment \"d08ba604\"\n",
      "    \n",
      "2022-09-07T21:50:16-07:00\n",
      "ID          = d08ba604\n",
      "Job ID      = webapp\n",
      "Job Version = 0\n",
      "Status      = running\n",
      "Description = Deployment is running\n",
      "\n",
      "Deployed\n",
      "Task Group  Desired  Placed  Healthy  Unhealthy  Progress Deadline\n",
      "webapp      6        6       0        0          2022-09-08T05:00:16Z\n"
     ]
    }
   ],
   "source": [
    "tail -n 50 /tmp/nomad_job_run.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eef16442-c617-4e8b-b72b-dd7a2ba09750",
   "metadata": {},
   "source": [
    "This should return something like this:\n",
    "\n",
    "```\n",
    "==> Monitoring evaluation \"a05672bc\"\n",
    "Evaluation triggered by job \"webapp\"\n",
    "Evaluation within deployment: \"5692a28d\"\n",
    "Allocation \"6bc9d9e6\" created: node \"33fd8505\", group \"webapp\"\n",
    "Allocation \"1b90c684\" created: node \"3006bb6d\", group \"webapp\"\n",
    "Allocation \"56b0671c\" created: node \"2f4a35ac\", group \"webapp\"\n",
    "Evaluation status changed: \"pending\" -> \"complete\"\n",
    "==> Evaluation \"a05672bc\" finished with status \"complete\"\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f218abe-8215-4d7e-b8c1-d9133fb1bda2",
   "metadata": {},
   "source": [
    "You can check the status of the job by selecting the \"`webapp`\" job on the \"Nomad UI\" tab."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "f5524c12-961a-424a-8e3a-b78d48342ab5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nomad UI: http://pphan-server-lb-992086560.us-west-2.elb.amazonaws.com:4646\n"
     ]
    }
   ],
   "source": [
    "printf \"%s\\n\" \"Nomad UI: $NOMAD_ADDR\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5eb2f4fc-08f0-406b-bf6e-951d10f9815a",
   "metadata": {},
   "source": [
    "After about 1 minute, you should see that the job has 6 healthy allocations, each representing a single instance of the web app."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adcc2aa1-306f-4f44-babd-0699d6f61c5d",
   "metadata": {},
   "source": [
    "Please also check the status of the job with the Nomad CLI:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "324c8f60-d478-4b1d-bfb0-5c95239becb5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0mID            = webapp\n",
      "Name          = webapp\n",
      "Submit Date   = 2022-09-07T21:50:16-07:00\n",
      "Type          = service\n",
      "Priority      = 50\n",
      "Datacenters   = dc1\n",
      "Namespace     = default\n",
      "Status        = running\n",
      "Periodic      = false\n",
      "Parameterized = false\u001b[0m\n",
      "\u001b[0m\n",
      "\u001b[1mSummary\u001b[0m\u001b[0m\u001b[0m\n",
      "\u001b[0mTask Group  Queued  Starting  Running  Failed  Complete  Lost  Unknown\n",
      "webapp      0       0         6        0       0         0     0\u001b[0m\n",
      "\u001b[0m\n",
      "\u001b[1mLatest Deployment\u001b[0m\u001b[0m\u001b[0m\n",
      "\u001b[0mID          = d08ba604\n",
      "Status      = running\n",
      "Description = Deployment is running\n",
      "\n",
      "\u001b[1mDeployed\u001b[0m\n",
      "Task Group  Desired  Placed  Healthy  Unhealthy  Progress Deadline\n",
      "webapp      6        6       1        0          2022-09-08T05:00:39Z\u001b[0m\u001b[0m\n",
      "\u001b[0m\n",
      "\u001b[1mAllocations\u001b[0m\u001b[0m\u001b[0m\n",
      "\u001b[0mID        Node ID   Task Group  Version  Desired  Status   Created  Modified\n",
      "33d4b95f  086e619b  webapp      0        run      running  25s ago  1s ago\n",
      "464cd457  160234a4  webapp      0        run      running  25s ago  8s ago\n",
      "577157b5  160234a4  webapp      0        run      running  25s ago  9s ago\n",
      "81407a40  160234a4  webapp      0        run      running  25s ago  8s ago\n",
      "a821c6ba  22f005c9  webapp      0        run      running  25s ago  6s ago\n",
      "f2906fdc  84c13196  webapp      0        run      running  25s ago  8s ago\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "nomad job status webapp"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0726e52d-a40b-4e1d-a9c2-3546305526cb",
   "metadata": {},
   "source": [
    "You can also inspect the \"`Consul UI`\" tab to see the health of the web app instances that have all been registered as services in Consul."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9ef7082-1f52-44fb-af01-43fbbb082c45",
   "metadata": {},
   "outputs": [],
   "source": [
    "printf \"%s\\n\" \"Consul UI: $CONSUL_HTTP_ADDR\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b659737c-6c53-4f38-ad51-373d69c7a4c9",
   "metadata": {},
   "source": [
    "- Click on the \"`webapp`\" service.\n",
    "    - Note how the instances are spread across the clients.\n",
    "- They might or might not be evenly distributed since we did not specify any job placement stanzas."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e3adb2c-e945-4edc-ae16-1a7826d4739f",
   "metadata": {},
   "source": [
    "## Inspect the traefik.nomad Job"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d74e3b75-8d9d-4740-b79c-c04ff79feca7",
   "metadata": {},
   "source": [
    "Inspect the \"`traefik.nomad`\" job specification file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "id": "9209d6db-19ed-4d2e-a239-e54d5663c3a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1]+  Done                    nomad job run webapp.nomad > /tmp/nomad_job_run.txt 2>&1\n"
     ]
    }
   ],
   "source": [
    "cat > /tmp/nomad_terraform/jobs/traefik.nomad <<-EOF\n",
    "job \"traefik\" {\n",
    "  region      = \"global\"\n",
    "  datacenters = [\"dc1\"]\n",
    "  type        = \"service\"\n",
    "  group \"traefik\" {\n",
    "    count = 1\n",
    "    network {\n",
    "      port \"http\" {\n",
    "        static = 8080\n",
    "      }\n",
    "      port \"api\" {\n",
    "        static = 8081\n",
    "      }\n",
    "    }\n",
    "    task \"traefik\" {\n",
    "      driver = \"docker\"\n",
    "      config {\n",
    "        image        = \"traefik:1.7\"\n",
    "        network_mode = \"host\"\n",
    "        volumes = [\n",
    "          \"local/traefik.toml:/etc/traefik/traefik.toml\",\n",
    "        ]\n",
    "      }\n",
    "      template {\n",
    "        data = <<EOD\n",
    "[entryPoints]\n",
    "    [entryPoints.http]\n",
    "    address = \":8080\"\n",
    "    [entryPoints.traefik]\n",
    "    address = \":8081\"\n",
    "[api]\n",
    "    dashboard = true\n",
    "# Enable Consul Catalog configuration backend.\n",
    "[consulCatalog]\n",
    "endpoint = \"127.0.0.1:8500\"\n",
    "domain = \"consul.localhost\"\n",
    "prefix = \"traefik\"\n",
    "constraints = [\"tag==service\"]\n",
    "EOD\n",
    "        destination = \"local/traefik.toml\"\n",
    "      }\n",
    "      resources {\n",
    "        cpu    = 250\n",
    "        memory = 128\n",
    "      }\n",
    "      service {\n",
    "        name = \"traefik\"\n",
    "        check {\n",
    "          name     = \"alive\"\n",
    "          type     = \"tcp\"\n",
    "          port     = \"http\"\n",
    "          interval = \"10s\"\n",
    "          timeout  = \"2s\"\n",
    "        }\n",
    "      }\n",
    "    }\n",
    "  }\n",
    "}\n",
    "EOF"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3fdb49d-53a6-4a06-b9c7-cf3f9b511eff",
   "metadata": {},
   "source": [
    "What does this job file do?\n",
    "- This will deploy a Docker container that runs Traefik\n",
    "- Traefik proxies all requests to the web app instances on port `8080` to their dynamic ports allocated by Nomad.\n",
    "- This job uses Nomad's [template](https://www.nomadproject.io/docs/job-specification/template/) stanza to write out a Traefik configuration file, \"`traefik.toml`\"\n",
    "    - Traefik will read this when started.\n",
    "    - The template includes Traefik's [constraints config](https://docs.traefik.io/providers/consul-catalog/#constraints) for Consul's services catalog with this setting:\n",
    "\n",
    "```\n",
    "constraints = [\"tag==service\"]\n",
    "```\n",
    "\n",
    "If you look back at the \"`webapp.nomad`\" job specification, on line 34 you will see that the same service tag was specified in the `tags` section of the registration of the web app with Consul.\n",
    "\n",
    "```\n",
    "tags = [\n",
    "\"traefik.tags=service\",\n",
    "\"traefik.frontend.rule=PathPrefixStrip:/myapp\",\n",
    "]\n",
    "```\n",
    "\n",
    "We think it's pretty cool that:\n",
    "- Nomad deploys both jobs\n",
    "- and registers them as Consul services\n",
    "- and that Traefik then uses the registrations of the web app instances with Consul to determine how to direct traffic to them."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfca852b-82ee-4a42-b0b9-da212e45f4a0",
   "metadata": {},
   "source": [
    "## Run the traefik.nomad Job\n",
    "\n",
    "Run the \"`traefik.nomad`\" job with this command on the \"`Server`\" tab:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "id": "947a58a3-e9c4-497e-b51c-eb6b8980a83c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1] 12380\n"
     ]
    }
   ],
   "source": [
    "nomad job run traefik.nomad > /tmp/nomad_job_run_traefik.txt 2>&1 &"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "id": "a13fcc6e-c4c8-498a-a257-34aef94f161b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==> 2022-09-07T21:50:57-07:00: Monitoring evaluation \"9eeacc3a\"\n",
      "    2022-09-07T21:50:57-07:00: Evaluation triggered by job \"traefik\"\n",
      "==> 2022-09-07T21:50:58-07:00: Monitoring evaluation \"9eeacc3a\"\n",
      "    2022-09-07T21:50:58-07:00: Evaluation within deployment: \"e75cc2d8\"\n",
      "    2022-09-07T21:50:58-07:00: Allocation \"acfa4d6d\" created: node \"160234a4\", group \"traefik\"\n",
      "    2022-09-07T21:50:58-07:00: Evaluation status changed: \"pending\" -> \"complete\"\n",
      "==> 2022-09-07T21:50:58-07:00: Evaluation \"9eeacc3a\" finished with status \"complete\"\n",
      "==> 2022-09-07T21:50:58-07:00: Monitoring deployment \"e75cc2d8\"\n",
      "    \n",
      "2022-09-07T21:50:58-07:00\n",
      "\n",
      "real\t0m0.001s\n",
      "user\t0m0.001s\n",
      "sys\t0m0.000s\n"
     ]
    }
   ],
   "source": [
    "time head -n 10 /tmp/nomad_job_run_traefik.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6d19d05-8ccc-4cff-acaf-0225851e26be",
   "metadata": {},
   "source": [
    "This should return something like this:\n",
    "\n",
    "```\n",
    "==> Monitoring evaluation \"6765c131\"\n",
    "    Evaluation triggered by job \"traefik\"\n",
    "    Evaluation within deployment: \"d15e6190\"\n",
    "    Allocation \"0e36e38a\" created: node \"44d88b4b\", group \"traefik\"\n",
    "    Evaluation status changed: \"pending\" -> \"complete\"\n",
    "==> Evaluation \"6765c131\" finished with status \"complete\"\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e85d14d9-d5e5-4c22-b5a3-76f2b8542df3",
   "metadata": {},
   "source": [
    "As before, you can check the status of the job by selecting the \"`traefik`\" job on the \"Nomad UI\" tab."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61b63de5-4345-4768-965f-cb82ac6da268",
   "metadata": {},
   "source": [
    "Check the status of the job with the Nomad CLI by running this command on the \"`Server`\" tab:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "id": "805f86c8-3df4-4a49-b319-4d7f2c0101c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0mID            = traefik\n",
      "Name          = traefik\n",
      "Submit Date   = 2022-09-07T21:50:57-07:00\n",
      "Type          = service\n",
      "Priority      = 50\n",
      "Datacenters   = dc1\n",
      "Namespace     = default\n",
      "Status        = running\n",
      "Periodic      = false\n",
      "Parameterized = false\u001b[0m\n",
      "\u001b[0m\n",
      "\u001b[1mSummary\u001b[0m\u001b[0m\u001b[0m\n",
      "\u001b[0mTask Group  Queued  Starting  Running  Failed  Complete  Lost  Unknown\n",
      "traefik     0       0         1        0       0         0     0\u001b[0m\n",
      "\u001b[0m\n",
      "\u001b[1mLatest Deployment\u001b[0m\u001b[0m\u001b[0m\n",
      "\u001b[0mID          = e75cc2d8\n",
      "Status      = running\n",
      "Description = Deployment is running\n",
      "\n",
      "\u001b[1mDeployed\u001b[0m\n",
      "Task Group  Desired  Placed  Healthy  Unhealthy  Progress Deadline\n",
      "traefik     1        1       0        0          2022-09-08T05:00:57Z\u001b[0m\u001b[0m\n",
      "\u001b[0m\n",
      "\u001b[1mAllocations\u001b[0m\u001b[0m\u001b[0m\n",
      "\u001b[0mID        Node ID   Task Group  Version  Desired  Status   Created  Modified\n",
      "acfa4d6d  160234a4  traefik     0        run      running  11s ago  7s ago\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "nomad job status traefik"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17eebfa6-c185-4729-aaf2-7808e6be8bac",
   "metadata": {},
   "source": [
    "**NEED TO MAKE APPLICABLE OUTSIDE OF INSTRUQT**\n",
    "\n",
    "Unfortunately, you cannot load the web app or Traefik UIs yet because we have not exposed Instruqt tabs for them.\n",
    "- In fact, we would have had to add tabs exposing port `8081` on all 3 Nomad clients in order to expose the Traefik dashboard since we could not predict in advance which Nomad client Traefik would be deployed to with the current \"`traefik.nomad`\" job specification. We will fix this in the next challenge."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db293194-4bd7-438a-95ff-8346175dbcc0",
   "metadata": {},
   "source": [
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b77bdc0-1a1c-4510-abcb-224361057ee5",
   "metadata": {},
   "source": [
    "- slug: use-constraint\n",
    "\n",
    "# Use Nomad's Constraint Stanza"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9fe8ba1-20c8-4ba0-9261-aed02ff019a0",
   "metadata": {},
   "source": [
    "teaser: |\n",
    "\n",
    "Use Nomad's constraint stanza to tightly control the placement of the Traefik job."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6767fc91-9f45-40df-ba39-e93eeaba7a96",
   "metadata": {},
   "source": [
    "## notes:\n",
    "\n",
    "In this challenge, you will update the Traefik job to run on a specific Nomad client node so that you can visit the Traefik Dashboard on a new Instruqt tab.\n",
    "\n",
    "You will do this by using Nomad's [constraint](https://www.nomadproject.io/docs/job-specification/constraint/) stanza that allows Nomad operators to tightly control the placement of a job's allocations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "599e1a96-041f-4699-ac89-fe949225918a",
   "metadata": {},
   "source": [
    "assignment:\n",
    "\n",
    "In this challenge, you will use Nomad's [constraint](https://www.nomadproject.io/docs/job-specification/constraint/) stanza to restrict Traefik to run on a specific Nomad client.\n",
    "\n",
    "We will be using a constraint that filters on a [node variable](https://www.nomadproject.io/docs/runtime/interpolation/#node-variables) of the Nomad client nodes, but you could also use [client metadata](https://www.nomadproject.io/docs/configuration/client#custom-metadata-network-speed-and-node-class)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c56f6b5-3ddc-41c0-861c-7c81dcb23d2b",
   "metadata": {},
   "source": [
    "Please navigate back to the `/tmp/nomad_terraform/jobs` directory on the \"`Server`\" tab:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "id": "556c4c40-7d46-485c-a90d-aad302e00d6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "cd /tmp/nomad_terraform/jobs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1376d7ec-e8e0-4e26-8d0b-6f1293c41858",
   "metadata": {},
   "source": [
    "## Edit the traefik.nomad Job Specification"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75d28d8a-9fe2-46a3-a58c-0aa8099f3105",
   "metadata": {},
   "source": [
    "Edit the \"`traefik.nomad`\" job specification file on the \"`Jobs`\" tab, making the following changes:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15cdc90c-4885-4fdf-b7fe-f0e2cc8183fe",
   "metadata": {},
   "source": [
    "First, grab a node to deploy the job to."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "id": "9f2bf3fa-0a10-4320-a379-8ca6620e38cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1]+  Done                    nomad job run traefik.nomad > /tmp/nomad_job_run_traefik.txt 2>&1\n",
      "ip-172-31-33-53\n"
     ]
    }
   ],
   "source": [
    "TRAEFIK_NODE=$(nomad node status | tail -n 1 | awk '{print $3}')\n",
    "echo $TRAEFIK_NODE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "672a3c8a-2e55-417b-8db2-028665d0c221",
   "metadata": {},
   "source": [
    "Find the line with `count = 1` and add the following constraint stanza after it:\n",
    "\n",
    "```go\n",
    "constraint {\n",
    "  attribute = \"${node.unique.name}\"\n",
    "  value     = \"client1\"\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "071256f6-ce53-4501-8bc1-9662526410b4",
   "metadata": {},
   "source": [
    "If you prefer, you can do the editing with this command on the \"`Server`\" tab:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "id": "2385938c-78d7-45ef-8d5f-cd97025ca7c0",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mv: cannot stat 'traefik.nomad.bak_20220907': No such file or directory\n",
      "total 8\n",
      "-rw-rw-r-- 1 pephan pephan  750 Sep  7 21:49 webapp.nomad\n",
      "-rw-rw-r-- 1 pephan pephan 1155 Sep  7 21:50 traefik.nomad\n",
      "job \"traefik\" {\n",
      "  region      = \"global\"\n",
      "  datacenters = [\"dc1\"]\n",
      "  type        = \"service\"\n",
      "  group \"traefik\" {\n",
      "    count = 1\n",
      "\n",
      "      constraint { \n",
      "         attribute = \"${node.unique.name}\"\n",
      "         value     = \"ip-172-31-33-53\"\n",
      "       }\n",
      "    network {\n",
      "      port \"http\" {\n",
      "        static = 8080\n",
      "      }\n",
      "      port \"api\" {\n",
      "        static = 8081\n",
      "      }\n",
      "    }\n",
      "    task \"traefik\" {\n",
      "      driver = \"docker\"\n",
      "      config {\n",
      "        image        = \"traefik:1.7\"\n",
      "        network_mode = \"host\"\n",
      "        volumes = [\n",
      "          \"local/traefik.toml:/etc/traefik/traefik.toml\",\n",
      "        ]\n",
      "      }\n",
      "      template {\n",
      "        data = <<EOD\n",
      "[entryPoints]\n",
      "    [entryPoints.http]\n",
      "    address = \":8080\"\n",
      "    [entryPoints.traefik]\n",
      "    address = \":8081\"\n",
      "[api]\n",
      "    dashboard = true\n",
      "# Enable Consul Catalog configuration backend.\n",
      "[consulCatalog]\n",
      "endpoint = \"127.0.0.1:8500\"\n",
      "domain = \"consul.localhost\"\n",
      "prefix = \"traefik\"\n",
      "constraints = [\"tag==service\"]\n",
      "EOD\n",
      "        destination = \"local/traefik.toml\"\n",
      "      }\n",
      "      resources {\n",
      "        cpu    = 250\n",
      "        memory = 128\n",
      "      }\n",
      "      service {\n",
      "        name = \"traefik\"\n",
      "        check {\n",
      "          name     = \"alive\"\n",
      "          type     = \"tcp\"\n",
      "          port     = \"http\"\n",
      "          interval = \"10s\"\n",
      "          timeout  = \"2s\"\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "mv traefik.nomad.bak_$(date +%Y%m%d) traefik.nomad\n",
    "ls -lrt\n",
    "sed -i\".bak_$(date +%Y%m%d)\" \"s/count = 1/count = 1\\n\\n\\\n",
    "      constraint { \\n \\\n",
    "        attribute = \\\"\\${node.unique.name}\\\"\\n \\\n",
    "        value     = \\\"${TRAEFIK_NODE}\\\"\\n \\\n",
    "      }/g\" \\\n",
    "  traefik.nomad\n",
    "cat traefik.nomad"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b555db8-6217-42ff-ac2e-8da4c7523b6b",
   "metadata": {},
   "source": [
    "## Re-run the traefik.nomad Job"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1807788b-06aa-4d5a-8115-aa904d4013b2",
   "metadata": {},
   "source": [
    "Next, re-run the \"`traefik.nomad`\" job with this command on the \"Server\" tab:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "id": "3ea98283-44a7-4126-beb4-73823d08565d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1] 12414\n"
     ]
    }
   ],
   "source": [
    "time nomad job run traefik.nomad > /tmp/nomad_job_run_traefik.txt 2>&1 &"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "id": "cb188c37-15d5-4c57-b895-4d7f2921a4f8",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==> 2022-09-07T21:51:38-07:00: Monitoring evaluation \"85b2258e\"\n",
      "    2022-09-07T21:51:38-07:00: Evaluation triggered by job \"traefik\"\n",
      "==> 2022-09-07T21:51:39-07:00: Monitoring evaluation \"85b2258e\"\n",
      "    2022-09-07T21:51:39-07:00: Evaluation within deployment: \"b34278a2\"\n",
      "    2022-09-07T21:51:39-07:00: Allocation \"acfa4d6d\" modified: node \"160234a4\", group \"traefik\"\n",
      "    2022-09-07T21:51:39-07:00: Evaluation status changed: \"pending\" -> \"complete\"\n",
      "==> 2022-09-07T21:51:39-07:00: Evaluation \"85b2258e\" finished with status \"complete\"\n",
      "==> 2022-09-07T21:51:39-07:00: Monitoring deployment \"b34278a2\"\n",
      "    \n",
      "2022-09-07T21:51:39-07:00\n"
     ]
    }
   ],
   "source": [
    "cat /tmp/nomad_job_run_traefik.txt | (head ; tail)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c1898c8-8ec4-4482-ad1b-447716bb008b",
   "metadata": {},
   "source": [
    "This should return something like this:<br>\n",
    "```\n",
    "==> Monitoring evaluation \"63a2e467\"\n",
    "    Evaluation triggered by job \"traefik\"\n",
    "    Evaluation within deployment: \"662516d9\"\n",
    "    Allocation \"b42c964c\" created: node \"99187f90\", group \"traefik\"\n",
    "    Evaluation status changed: \"pending\" -> \"complete\"\n",
    "==> Evaluation \"63a2e467\" finished with status \"complete\"\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ad50c50-8256-416e-ab8e-919dd49f4c74",
   "metadata": {},
   "source": [
    "- Look at the \"`traefik`\" job in the Nomad UI\n",
    "  - You will see that there is 1 allocation currently `running`.\n",
    "  - Click on the ID of that allocation in the `Client` column.\n",
    "  - You will be taken to the \"`client1`\" node.\n",
    "\n",
    "This shows that the `constraint` worked as desired."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6e78e2b-28d0-4c74-80e2-a7218a824cb0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#DEBUGGING\n",
    "nomad node status\n",
    "nomad job status traefik"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48603eb3-4b60-40ba-80ec-99404b470891",
   "metadata": {},
   "source": [
    "### **NEED TO ADD RULE TO ALLOW ALL FROM MY IP**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92d503f4-48c3-4e41-b8d1-d57a173fad83",
   "metadata": {},
   "source": [
    "## Verify"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a78dbf78-a784-49ed-9c5a-20a6f038421e",
   "metadata": {},
   "source": [
    "Now, you can visit the Traefik dashboard on the \"`Traefik UI`\" tab."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "id": "675bc3f4-bdb0-45e2-baf0-245923308535",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Traefik UI: http://34.216.150.37:8081\n"
     ]
    }
   ],
   "source": [
    "TRAEFIK_NODE_PUBLIC=$(aws ec2 describe-instances \\\n",
    "  | jq -r \".Reservations[].Instances[] \\\n",
    "  | select(.PrivateDnsName | contains(\\\"${TRAEFIK_NODE}\\\")) | .PublicIpAddress\")\n",
    "\n",
    "echo Traefik UI: http://${TRAEFIK_NODE_PUBLIC}:8081"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50cfd02c-49e1-400f-9822-0041ce811fa4",
   "metadata": {},
   "source": [
    "- You can see the URLs for the 6 instances of the web app that it has registered.\n",
    "- This tab was pre-configured to point to the `nomad-client-1` tab since we knew in advance the `constraint` you would use.\n",
    "- It accesses that node on port `8081` which is Traefik's admin port.\n",
    "\n",
    "Right-click any of those URLs in the \"`backend-webapp`\" table\n",
    "- Select \"`Copy Link Address`\".\n",
    "- Then run a command like this\n",
    "```shell\n",
    "curl <your_url>\n",
    "```\n",
    "    - where `<your_url>` is the URL you copied."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "id": "86c44b2a-f534-4343-ae6e-46a259b2c4e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "  0     0    0     0    0     0      0      0 --:--:-- --:--:Welcome! You are on node 172.31.33.53:22616\n",
      "100    44  100    44    0     0  65281      0 --:--:-- --:--:-- --:--:-- 44000\n"
     ]
    }
   ],
   "source": [
    "ssh -i /tmp/Nomad/ssh_key/aws-key-pair \\\n",
    "  -o \"StrictHostKeyChecking no\" ubuntu@${TRAEFIK_NODE_PUBLIC} \\\n",
    "  curl http://172.31.33.53:22616"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a7f3e1d-2505-4cf0-a494-2f46c4e938c4",
   "metadata": {},
   "source": [
    "- You should see something like this:<br>\n",
    "`Welcome! You are on node 10.132.0.66:20478`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9895cd29-4f67-424c-9866-7e2e5d4531d8",
   "metadata": {},
   "source": [
    "By specifying the IP and the port that Nomad dynamically selected, you are hitting one of the webapp allocations directly just as Traefik does."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b510e571-8808-4a55-9f05-20d1d22d6ec0",
   "metadata": {},
   "source": [
    "Next, run the following `curl` command:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "id": "abc18449-8233-4cb0-8aeb-803d706e0844",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Welcome! You are on node 172.31.36.154:28165\n",
      "Welcome! You are on node 172.31.33.53:22616\n",
      "Welcome! You are on node 172.31.33.53:25028\n",
      "Welcome! You are on node 172.31.33.53:26874\n",
      "Welcome! You are on node 172.31.36.101:20772\n",
      "Welcome! You are on node 172.31.36.154:27310\n",
      "http://34.216.150.37:8080/myapp\n"
     ]
    }
   ],
   "source": [
    "for i in {1..6}; do\n",
    "curl http://${TRAEFIK_NODE_PUBLIC}:8080/myapp\n",
    "done\n",
    "echo http://${TRAEFIK_NODE_PUBLIC}:8080/myapp"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a46b75f2-e9ec-4b02-b2e5-06fadbfe6ebd",
   "metadata": {},
   "source": [
    "This will return a similar message.\n",
    "\n",
    "In this case, you are actually hitting Traefik on Nomad client 1 and it is load balancing your request to one of the 6 webapp instances. If you repeat the command a few times, you will see that the IP and port returned are different each time."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d643204-deee-4eaf-b6ce-4b8dce23fa76",
   "metadata": {},
   "source": [
    "You can also visit the web app on the \"`Web App UI`\" tab.\n",
    "- This tab also points to the `nomad-client-1` node but listens on port `8080` which is what Traefik is using to load balance requests to the web app.\n",
    "- You will see the same message that the `curl` command gave.\n",
    "- If you click the Instruqt refresh button (clockwise arrow) to the right of the \"`Web App UI`\" tab, the IP and port displayed will also change.\n",
    "\n",
    "In the next challenge, you will use Nomad's spread stanza to distribute the allocations of your \"`webapp.nomad`\" job evenly across your 3 Nomad clients.\n",
    "\n",
    "tabs:\n",
    "- title: Traefik UI\n",
    "type: service\n",
    "hostname: nomad-client-1\n",
    "port: 8081\n",
    "- title: Web App UI\n",
    "type: service\n",
    "hostname: nomad-client-1\n",
    "path: /myapp\n",
    "port: 8080\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a982961-d8b1-4071-a6fb-07f459c3da5e",
   "metadata": {},
   "source": [
    "- slug: use-spread\n",
    "  \n",
    "# Use Nomad's Spread stanza"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fa4b144-47cf-4e7d-889f-928cb55df868",
   "metadata": {},
   "source": [
    "  teaser: |\n",
    "    Use Nomad's spread stanza to distribute load evenly across your Nomad clients."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "113648bd-6cf3-42e4-996d-a9e9652d6b4c",
   "metadata": {},
   "source": [
    "notes:\n",
    "\n",
    "In this challenge, you will update the logic that Nomad uses to distribute allocations of the web app to the 3 Nomad clients in your cluster.\n",
    "\n",
    "Specifically, you will use the [spread](https://www.nomadproject.io/docs/job-specification/spread/) stanza to evenly distribute allocations of the web app across all 3 Nomad clients."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c21efb52-117f-4c42-ad67-f2f122bfe7c3",
   "metadata": {},
   "source": [
    "assignment: |-\n",
    "\n",
    "In this challenge, you will use Nomad's [spread](https://www.nomadproject.io/docs/job-specification/spread/) stanza to spread the \"`webapp.nomad`\" job's allocations evenly across the 3 Nomad clients of your cluster.\n",
    "\n",
    "This demonstrates how Nomad can increase the failure tolerance of applications.\n",
    "\n",
    "The `spread` stanza allows operators to spread allocations over datacenters, availability zones, or even racks in a physical datacenter. By default, when using `spread`, the scheduler will attempt to place allocations equally among the available values of the given target."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7877d4d-6ef4-40a5-97e4-619cd64cc0a7",
   "metadata": {},
   "source": [
    "## Edit the webapp.nomad Job"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae66ab1f-c324-4fa1-9a51-f47d7290a9e9",
   "metadata": {},
   "source": [
    "Navigate back to the `/tmp/nomad/jobs` directory on the \"`Server`\" tab:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "id": "1debe1b0-51aa-4781-ba87-a332a5028e70",
   "metadata": {},
   "outputs": [],
   "source": [
    "cd /tmp/nomad_terraform/jobs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d08207b1-9127-4c28-aecf-1ddde1f24efb",
   "metadata": {},
   "source": [
    "Edit the \"`webapp.nomad`\" job specification file on the \"`Jobs`\" tab, making the following changes:\n",
    "\n",
    "Find the line that has `count = 6` and add the following spread stanza after it:\n",
    "\n",
    "```go\n",
    "spread {\n",
    "  attribute = \"${node.unique.name}\"\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dab10cd-afae-43e0-b033-3200740c76b1",
   "metadata": {},
   "source": [
    "If you prefer, you can do the editing with this command on the \"`Server`\" tab:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "id": "fc036e5f-7df2-455b-835f-9c33a6663ec4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mv: cannot stat 'webapp.nomad.bak_20220907': No such file or directory\n",
      "total 12\n",
      "-rw-rw-r-- 1 pephan pephan  750 Sep  7 21:49 webapp.nomad\n",
      "-rw-rw-r-- 1 pephan pephan 1155 Sep  7 21:50 traefik.nomad.bak_20220907\n",
      "-rw-rw-r-- 1 pephan pephan 1267 Sep  7 21:51 traefik.nomad\n",
      "job \"webapp\" {\n",
      "  datacenters = [\"dc1\"]\n",
      "  group \"webapp\" {\n",
      "    count = 6\n",
      "\n",
      "    spread { \n",
      "      attribute = \"${node.unique.name}\" \n",
      "    }\n",
      "    network {\n",
      "      port  \"http\" {}\n"
     ]
    }
   ],
   "source": [
    "mv webapp.nomad.bak_$(date +%Y%m%d) webapp.nomad\n",
    "ls -lrt\n",
    "\n",
    "sed -i\".bak_$(date +%Y%m%d)\" \\\n",
    "  's/count = 6/count = 6\\n\\\n",
    "    spread { \\\n",
    "      attribute = \"${node.unique.name}\" \\\n",
    "    }/g' \\\n",
    "  webapp.nomad\n",
    "head -n 10  webapp.nomad"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcedd2b4-02e7-42e4-9354-ea576fbee9ca",
   "metadata": {},
   "source": [
    "**NOTE**:\n",
    "- We do not specify a `value` the way we did in the `constraint` stanza in the last challenge.\n",
    "- The whole point here is to spread allocations evenly across all Nomad clients based on their names.\n",
    "\n",
    "**PP - ADD MORE INFO**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c69eaf1a-574a-414a-b43c-a57957d9f2a6",
   "metadata": {},
   "source": [
    "You can view the current allocations for the \"`webapp`\" job.\n",
    "- select the `webapp` job in the Nomad UI\n",
    "- click on the \"`Allocations`\" tab under the job\n",
    "- In the Nomad UI, focus on the \"`Client`\" column.\n",
    "- CLI\n",
    "    - or by running `nomad job status webapp`\n",
    "    - and looking at the \"`Allocations`\" section at the bottom of the output.\n",
    "    ```text\n",
    "    ...\n",
    "    Allocations\n",
    "    ID        Node ID   Task Group  Version  Desired  Status   Created    Modified\n",
    "    36655ee2  69bb47a1  webapp      0        run      running  13m8s ago  12m29s ago\n",
    "    37ea648c  71670de6  webapp      0        run      running  13m8s ago  12m32s ago\n",
    "    49874897  f3d852dd  webapp      0        run      running  13m8s ago  12m33s ago\n",
    "    73498c1a  71670de6  webapp      0        run      running  13m8s ago  12m31s ago\n",
    "    90263f17  69bb47a1  webapp      0        run      running  13m8s ago  12m30s ago\n",
    "    cddf807a  69bb47a1  webapp      0        run      running  13m8s ago  12m28s ago    \n",
    "    ```\n",
    "    - Focus on the \"`Node ID`\" column. The allocations might or might not be evenly distributed across the 3 Nomad clients"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc528850-cb70-4ece-9812-9617b9ad6215",
   "metadata": {},
   "source": [
    "## Re-run the webapp.nomad job"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a197402-50dd-48b7-8429-784a4cbe6f0b",
   "metadata": {},
   "source": [
    "Now let's re-run the \"`webapp.nomad`\" job and see the changes that occur:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "id": "3aa77da4-2162-467d-9e1b-d5807efdfd61",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1] 12580\n"
     ]
    }
   ],
   "source": [
    "time nomad job run webapp.nomad > /tmp/nomad_job_run.txt 2>&1 &"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "id": "ce738801-65f8-428e-8ecb-bee74d0c24a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==> 2022-09-07T22:06:36-07:00: Monitoring evaluation \"0a89b585\"\n",
      "    2022-09-07T22:06:36-07:00: Evaluation triggered by job \"webapp\"\n",
      "==> 2022-09-07T22:06:37-07:00: Monitoring evaluation \"0a89b585\"\n",
      "    2022-09-07T22:06:37-07:00: Evaluation within deployment: \"3c970f9e\"\n",
      "    2022-09-07T22:06:37-07:00: Allocation \"0a5bddcf\" created: node \"84c13196\", group \"webapp\"\n",
      "    2022-09-07T22:06:37-07:00: Evaluation status changed: \"pending\" -> \"complete\"\n",
      "==> 2022-09-07T22:06:37-07:00: Evaluation \"0a89b585\" finished with status \"complete\"\n",
      "==> 2022-09-07T22:06:37-07:00: Monitoring deployment \"3c970f9e\"\n",
      "    \n",
      "2022-09-07T22:06:37-07:00\n",
      "\n",
      "\n",
      "Deployed\n",
      "Task Group  Desired  Placed  Healthy  Unhealthy  Progress Deadline\n",
      "webapp      6        2       1        0          2022-09-08T05:16:48Z\n",
      "    \n",
      "2022-09-07T22:07:01-07:00\n",
      "ID          = 3c970f9e\n",
      "Job ID      = webapp\n",
      "Job Version = 1\n",
      "Status      = running\n",
      "Description = Deployment is running\n",
      "\n",
      "Deployed\n",
      "Task Group  Desired  Placed  Healthy  Unhealthy  Progress Deadline\n",
      "webapp      6        2       1        0          2022-09-08T05:16:48Z\n"
     ]
    }
   ],
   "source": [
    "(head -n 10 ; echo ; tail -n 15) < /tmp/nomad_job_run.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4cddebc-0ae5-4848-b3a9-9253621ae30e",
   "metadata": {},
   "source": [
    "- Monitor the new deployment of the job in the Nomad UI\n",
    "- Or by periodically re-running `nomad job status webapp`.\n",
    "- Pay particular attention to `running` allocations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "id": "21a6fecd-4b88-4d61-9ed7-c25d7112ef5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0mID            = webapp\n",
      "Name          = webapp\n",
      "Submit Date   = 2022-09-07T22:06:36-07:00\n",
      "Type          = service\n",
      "Priority      = 50\n",
      "Datacenters   = dc1\n",
      "Namespace     = default\n",
      "Status        = running\n",
      "Periodic      = false\n",
      "Parameterized = false\u001b[0m\n",
      "\u001b[0m\n",
      "\u001b[1mSummary\u001b[0m\u001b[0m\u001b[0m\n",
      "\u001b[0mTask Group  Queued  Starting  Running  Failed  Complete  Lost  Unknown\n",
      "webapp      0       1         5        0       3         1     0\u001b[0m\n",
      "\u001b[0m\n",
      "\u001b[1mLatest Deployment\u001b[0m\u001b[0m\u001b[0m\n",
      "\u001b[0mID          = 3c970f9e\n",
      "Status      = running\n",
      "Description = Deployment is running\n",
      "\n",
      "\u001b[1mDeployed\u001b[0m\n",
      "Task Group  Desired  Placed  Healthy  Unhealthy  Progress Deadline\n",
      "webapp      6        3       2        0          2022-09-08T05:17:03Z\u001b[0m\u001b[0m\n",
      "\u001b[0m\n",
      "\u001b[1mAllocations\u001b[0m\u001b[0m\u001b[0m\n",
      "\u001b[0mID        Node ID   Task Group  Version  Desired  Status    Created     Modified\n",
      "84399f09  22f005c9  webapp      1        run      pending   1s ago      1s ago\n",
      "36fcc5d1  84c13196  webapp      1        run      running   16s ago     3s ago\n",
      "0a5bddcf  84c13196  webapp      1        run      running   31s ago     18s ago\n",
      "af302c8a  22f005c9  webapp      0        run      running   2m13s ago   2m1s ago\n",
      "33d4b95f  086e619b  webapp      0        stop     lost      16m51s ago  2m13s ago\n",
      "464cd457  160234a4  webapp      0        stop     complete  16m51s ago  1s ago\n",
      "577157b5  160234a4  webapp      0        run      running   16m51s ago  16m24s ago\n",
      "81407a40  160234a4  webapp      0        run      running   16m51s ago  16m22s ago\n",
      "a821c6ba  22f005c9  webapp      0        stop     complete  16m51s ago  30s ago\n",
      "f2906fdc  84c13196  webapp      0        stop     complete  16m51s ago  16s ago\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "nomad job status webapp"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dc332f8-feb9-4126-9763-c7962b5ec11c",
   "metadata": {},
   "source": [
    "After all six allocations are healthy, you should see 2 webapp allocations on each Nomad client."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8eec81c-bb42-4b21-a709-1353ba819fa2",
   "metadata": {},
   "source": [
    "Sample Output\n",
    "```shell\n",
    "Allocations\n",
    "ID        Node ID   Task Group  Version  Desired  Status    Created     Modified\n",
    "6d4e5a11  69bb47a1  webapp      1        run      running   8m ago      7m48s ago\n",
    "83dd87eb  71670de6  webapp      1        run      running   8m16s ago   8m1s ago\n",
    "c74711fe  f3d852dd  webapp      1        run      running   8m29s ago   8m17s ago\n",
    "2e6adccf  69bb47a1  webapp      1        run      running   8m44s ago   8m31s ago\n",
    "1fdb24b9  71670de6  webapp      1        run      running   8m58s ago   8m45s ago\n",
    "a1340eb4  f3d852dd  webapp      1        run      running   9m13s ago   9m ago\n",
    "36655ee2  69bb47a1  webapp      0        stop     complete  23m33s ago  8m ago\n",
    "73498c1a  71670de6  webapp      0        stop     complete  23m33s ago  8m58s ago\n",
    "90263f17  69bb47a1  webapp      0        stop     complete  23m33s ago  8m43s ago\n",
    "37ea648c  71670de6  webapp      0        stop     complete  23m33s ago  8m15s ago\n",
    "49874897  f3d852dd  webapp      0        stop     complete  23m33s ago  8m29s ago\n",
    "cddf807a  69bb47a1  webapp      0        stop     complete  23m33s ago  9m13s ago\n",
    "```\n",
    "\n",
    "This shows that the `spread` stanza caused Nomad to spread the allocations evenly as expected.\n",
    "\n",
    "In the next challenge, you will use the `affinity` stanza to express your preference on where Nomad should run the webapp allocations.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14045856-f766-42f3-a2ad-9701b39c536a",
   "metadata": {},
   "source": [
    "- slug: use-affinity\n",
    "\n",
    "# Use Nomad's Affinity Stanza"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea47bc04-9b61-4fd1-9363-29a99c9d84e7",
   "metadata": {},
   "source": [
    "teaser: |\n",
    "Use Nomad's affinity stanza to to loosely control the placement of jobs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10f9deff-bb5b-42c3-9879-3539ee046828",
   "metadata": {},
   "source": [
    "## notes:\n",
    "\n",
    "In this challenge, you will use Nomad's [affinity](https://www.nomadproject.io/docs/job-specification/affinity/) stanza to loosely control the placement of the \"`webapp`\" job.\n",
    "\n",
    "You will specify a preference on where Nomad should run the job's allocations but let Nomad make the final decision which will factor in your affinity preferences along with Nomad's default job anti-affinity and bin packing algorithms."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e1bf002-8116-476c-9340-93bac156bee4",
   "metadata": {},
   "source": [
    "assignment:\n",
    "\n",
    "In this challenge, you will use Nomad's [affinity](https://www.nomadproject.io/docs/job-specification/affinity/) stanza to loosely control the placement of the \"`webapp`\" job. You will specify a preference on where Nomad should run the job's allocations but let Nomad make the final decision.\n",
    "\n",
    "The `affinity` stanza allows operators to express placement preference for a set of nodes. Affinities may be expressed on attributes or client metadata. Additionally, affinities may be specified at the `job`, `group`, or `task` levels for ultimate flexibility.\n",
    "\n",
    "For this challenge we will be utilizing the underlying host machine type to choose where to run the allocations of the \"`webapp`\" job. The machine types are as follows:\n",
    "\n",
    "| client | type |\n",
    "| ------- | --- |\n",
    "| client1 | n1-standard-2\n",
    "| client2 | n1-standard-1\n",
    "| client3 | n1-standard-1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2c3c8d0-2623-440c-88f8-08f977bb0be8",
   "metadata": {},
   "source": [
    "## Edit the webapp.nomad Job"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce47ea88-81a7-4d08-9a5e-9a4c8d0d2005",
   "metadata": {},
   "source": [
    "Please navigate back to the `/root/nomad/jobs` directory on the \"`Server`\" tab:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "id": "6655c283-d0ce-4613-89cd-3f60a404f539",
   "metadata": {},
   "outputs": [],
   "source": [
    "cd /tmp/nomad_terraform/jobs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7907ce41-6ce9-4bbd-b55b-f8a4074934f0",
   "metadata": {},
   "source": [
    "Edit the \"`webapp.nomad`\" job specification file on the \"`Jobs`\" tab, making the following changes:\n",
    "\n",
    "Find the spread stanza and replace it with the following affinity stanza:\n",
    "```go\n",
    "affinity {\n",
    "  attribute = \"${attr.platform.gce.machine-type}\"\n",
    "  value     = \"n1-standard-2\"\n",
    "  weight    = 100\n",
    "}\n",
    "```\n",
    "\n",
    "This tells Nomad that you would like it to deploy all allocations of the \"`webapp`\" job to the \"`client1`\" node\n",
    "- since that is the only Nomad client using the \"`n1-standard-2`\" machine type.\n",
    "- We have set the `weight` of the affinity stanza to the highest possible value, `100`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5aa8fa29-e121-497f-8f1f-0e7bae9fee4b",
   "metadata": {},
   "source": [
    "If you prefer, you can do the editing with these commands on the \"`Server`\" tab:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1511ce44-a3d4-4aeb-99e1-ff020b8ed17f",
   "metadata": {},
   "outputs": [],
   "source": [
    "mv webapp.nomad.bak2_$(date +%Y%m%d) webapp.nomad\n",
    "sed -i\".bak2_$(date +%Y%m%d)\" '6,8d' webapp.nomad   #delete spread\n",
    "\n",
    "sed -i \\\n",
    "  's/count = 6/count = 6\\n\\\n",
    "      affinity { \\\n",
    "        attribute = \"${attr.platform.gce.machine-type}\" \\\n",
    "        value     = \"n1-standard-2\" \\\n",
    "        weight    = 100 \\\n",
    "      }/g' \\\n",
    "  webapp.nomad\n",
    "head -n 20 webapp.nomad"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a9be957-dd94-46c9-9769-617fb7b23f81",
   "metadata": {},
   "source": [
    "**NOTE**: Negative weights can be specified to indicate \"`anti-affinities`\".\n",
    "\n",
    "To make it easier to track the new deployment of the \"`webapp`\" job, let's first stop it with this command:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0017cf3e-3c9b-4440-beab-460d08918e21",
   "metadata": {},
   "outputs": [],
   "source": [
    "nomad job stop -purge webapp"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da7a33a4-20f3-4d76-a9d7-f3103d6581e1",
   "metadata": {},
   "source": [
    "This will completely remove the \"webapp\" job from the list of jobs in the Nomad UI."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "313c6448-df29-4d3e-b0c2-0fb5a0914cb2",
   "metadata": {},
   "source": [
    "## Re-run the webapp.nomad job with affinity"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa9acc4d-0bca-4e1e-a8d9-16899d2bbe92",
   "metadata": {},
   "source": [
    "Now, let's re-run the \"`webapp`\" job again:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d19df582-03f1-4d7f-9340-10d5d39bcebc",
   "metadata": {},
   "outputs": [],
   "source": [
    "time nomad job run webapp.nomad > /tmp/nomad_job_run.txt 2>&1 &"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5a92271-5dda-4b99-b6af-994f862ca5ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "(head -n 10;tail -n 10) < /tmp/nomad_job_run.txt\n",
    "# cat /tmp/nomad_job_run.txt | (sed -u 10q; echo; tail -n 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef53bc28-b7e6-4be9-8deb-d443c98fe997",
   "metadata": {},
   "source": [
    "The job should deploy 6 new allocations, but probably will not deploy all of them to the \"`client1`\" node as you had requested.\n",
    "\n",
    "You can check where the allocations were actually deployed by inspecting the \"`webapp`\" job in the the Nomad UI and looking at the \"`Allocations`\" tab of the job.\n",
    "- You can sort the allocations by clicking on the \"`Status`\" column header until all the running allocations are at the top."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "044e133f-be99-4a13-a756-71863e349c51",
   "metadata": {},
   "source": [
    "Check its status with the Nomad CLI using this command:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb75deb5-69c8-4b18-a639-cbf24c69587f",
   "metadata": {},
   "outputs": [],
   "source": [
    "nomad job status webapp"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f16c680-90d1-4751-a28a-ab7f0d99819c",
   "metadata": {},
   "source": [
    "Then, get detailed information on how Nomad decided where to deploy one of the allocations that was deployed to `client1`:\n",
    "```shell\n",
    "nomad alloc status -verbose <alloc>\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50bb7552-026c-4f1e-af98-5a05007df1b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "nomad alloc status -verbose $(nomad job status webapp | tail -n 5 \\\n",
    "  | sort -k 2 | sed -n '3p' | awk '{print $1}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c02f249c-0550-4e1e-b2d0-b1edf139e3d8",
   "metadata": {},
   "source": [
    "- replacing `<alloc>` with one of the allocation IDs in the first column of the \"`Allocations`\" section at the bottom of the output for which the corresponding Node ID matches the ID of the \"`client1`\" node.\n",
    "- You can determine the Node ID of `client1` on the `Clients` section of the Nomad UI."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "023f2f48-468f-492a-8eba-b487b556098f",
   "metadata": {},
   "source": [
    "If you look at the \"`Placement Metrics`\" section at the bottom, you will see various scores for each of the 3 Nomad clients. See this [section](https://www.nomadproject.io/docs/job-specification/affinity/#example-placement-metadata) for an explanation of the scores.\n",
    "\n",
    "Sample Output\n",
    "```text\n",
    "Placement Metrics\n",
    "Node                    binpack  job-anti-affinity  node-affinity  node-reschedule-penalty  final score\n",
    "e4818dac-a9a3-500c-...  0.458    -0.833             1              0                        0.208\n",
    "0acc8763-ee83-4ad6-...  0.203    0                  0              0                        0.203\n",
    "e42317e7-510d-95a0-...  0.203    0                  0              0                        0.203\n",
    "```\n",
    "\n",
    "The placement score is affected by the following factors.\n",
    "\n",
    "- `bin-packing` - Scores nodes according to how well they fit requirements.\n",
    "    - Optimizes for using minimal number of nodes.\n",
    "- `job-anti-affinity` - A penalty added for additional instances of the same job on a node, used to avoid having too many instances of a job on the same node.\n",
    "- `node-reschedule-penalty` - Used when the job is being rescheduled.\n",
    "    - Nomad adds a penalty to avoid placing the job on a node where it has failed to run before.\n",
    "- `node-affinity` - Used when the criteria specified in the affinity stanza matches the node."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffd0446c-3697-40ca-a53c-211f09f012dc",
   "metadata": {},
   "source": [
    "There are several reasons why Nomad might not deploy all allocations according to your `affinity` stanza's preferences:\n",
    "* Nomad automatically applies a job anti-affinity rule which discourages co-locating multiple instances of a task group.\n",
    "* Nomad applies a bin packing algorithm that attempts to optimize the resource utilization and density of applications in order to leave large blocks of resources available on some Nomad clients in case a future job attempts to schedule allocations that require large amounts of memory and CPU.\n",
    "\n",
    "You can read more about both of these concepts in Nomad's [Scheduling](https://www.nomadproject.io/docs/internals/scheduling/scheduling/) documentation.\n",
    "\n",
    "Congratulations on completing the Nomad Advanced Job Placement track!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd9e3da3-89ef-4549-ad68-c91f7618a14f",
   "metadata": {},
   "source": [
    "### Command Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ec2b154-40b1-4dab-8f24-22afb6119b0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "nomad job stop -purge webapp\n",
    "nomad job run webapp.nomad\n",
    "nomad job status webapp\n",
    "nomad alloc status -verbose $(nomad job status webapp | tail -n 5 \\\n",
    "  | sort -k 2 | sed -n '3p' | awk '{print $1}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd396890-0038-4d74-9c12-78f134a4f2a1",
   "metadata": {},
   "source": [
    "# Clean Up"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd977e44-20d0-4acc-a9ed-82277d35c8c6",
   "metadata": {
    "tags": []
   },
   "source": [
    "### terraform destroy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "814888ec-e3ec-4efe-a426-f6aff2e84814",
   "metadata": {},
   "outputs": [],
   "source": [
    "pushd /tmp/nomad_terraform/aws/env\n",
    "time terragrunt destroy > /tmp/tf_destroy_nomad_out.txt 2>&1 &\n",
    "popd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbf3154a-4fd8-4355-9a87-8dbee0fb8adb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "tail -n 50 /tmp/tf_destroy_nomad_out.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdeaf090-aec0-4819-a3f0-ac26a72fe498",
   "metadata": {},
   "outputs": [],
   "source": [
    "rm /tmp/Nomad/cluster/terraform.tfstate*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b84994d-7586-4bee-bf16-d86166e6411c",
   "metadata": {},
   "source": [
    "# Q&A"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4392d9d0-b19f-4e91-8a33-a6058c1a731e",
   "metadata": {},
   "source": [
    "Hi SMEs! Is there any documentation around the behavior of updating the [scheduling algorithm](https://www.nomadproject.io/api-docs/operator/scheduler#update-scheduler-configuration) on a Nomad cluster from binpack to spread. Specifically, are allocs spread when redeployed or does Nomad try to balance things proactively after the setting is applied? :thank-you-2:\n",
    "\n",
    "\n",
    "\n",
    "Correct, the scheduling change is only forward looking. The best answer to get things spread after the fact is a set of rolling-drains.\n",
    ":thank-you-2:\n",
    "1\n",
    "\n",
    "\n",
    "\n",
    "Daniel Santos\n",
    ":knife_fork_plate:  1 day ago\n",
    ":this-really:  We did this in our Prod Nomad clusters a couple weeks ago and the spread behaviour applies only after new allocs / evals are placed"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Bash",
   "language": "bash",
   "name": "bash"
  },
  "language_info": {
   "codemirror_mode": "shell",
   "file_extension": ".sh",
   "mimetype": "text/x-sh",
   "name": "bash"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
